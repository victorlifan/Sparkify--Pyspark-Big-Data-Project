{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project overview\n",
    "Imagine you are working on the data team for a popular digital music service similar to Spotify or Pandora. Many of users stream their favorite songs to your service every day either using the free tier that place advertisements between the songs or using the premium subscription model, where they stream music as free but pay a monthly flat rate.\n",
    "\n",
    "Users can upgrade, downgrade, or cancel their service at any time. So, it's crucial to make sure your users love the service. Every time a user interacts with the service while they're playing songs, logging out, like in a song with a thumbs up, hearing an ad, or downgrading their service, it generates data. All this data contains the key insights for keeping your users happy and helping your business thrive. (The full dataset is 12GB)\n",
    "\n",
    "It's your job on the data team to predict which users are at risk to churn either downgrading from premium to free tier or cancelling their service altogether. If you can accurately identify these users before they leave, your business can offer them discounts and incentives, potentially saving your business millions in revenue.\n",
    "\n",
    "To tackle this project, you will need to load, explorer, and clean this dataset with Spark. Based on your explanation, you will create features and build models with Spark to predict which users were churn from your digital music service.\n",
    "\n",
    "\n",
    "Predicting churn rates is a challenging and common problem that data scientists and analysts regularly encounter in any customer-facing business. Additionally, the ability to efficiently manipulate large datasets with Spark is one of the highest-demand skills in the field of data.\n",
    "\n",
    "# Problem Statement\n",
    "* Load large datasets into Spark and manipulate them using Spark SQL and Spark Dataframes\n",
    "* Gain insight of data set for better understanding (Data Exploration, Data Visualization etc.)\n",
    "* Data preprocessing\n",
    "* Use the machine learning APIs within Spark ML to build and tune models\n",
    "* Predicting which users are at risk to churn and validate predict result\n",
    "\n",
    "# Metrics\n",
    "Some of the metrics we might use:\n",
    "* Accuracy, recall, prescision, fmeasure(f1 socre)\n",
    "* Coefficient\n",
    "\n",
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini-sparkify-event-data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# refer 'https://changhsinlee.com/install-pyspark-windows-jupyter/#comment-4302741820'\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "plt.rcParams['figure.figsize']=(15,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upzip files\n",
    "with zipfile.ZipFile(\"data_zip/mini_sparkify_event_data.zip\",'r') as zf:\n",
    "    zf.extractall(\"../Sparkify--Pyspark-Big-Data-Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()\n",
    "# Read in full sparkify dataset\n",
    "event_data = \"mini_sparkify_event_data.json\"\n",
    "df = spark.read.json(event_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "| Submit Registration|\n",
      "|            Settings|\n",
      "|               Login|\n",
      "|            Register|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for differnet pages\n",
    "df.createOrReplaceTempView('t1')\n",
    "spark.sql('''\n",
    "select distinct page\n",
    "from t1''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|userAgent                                                                                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"                 |\n",
      "|\"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"                                   |\n",
      "|Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:31.0) Gecko/20100101 Firefox/31.0                                                                 |\n",
      "|\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"                 |\n",
      "|\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36\"                 |\n",
      "|Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:31.0) Gecko/20100101 Firefox/31.0                                                          |\n",
      "|Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0                                                                   |\n",
      "|Mozilla/5.0 (Windows NT 6.3; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0                                                                   |\n",
      "|Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)                                                                     |\n",
      "|\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"                            |\n",
      "|Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Firefox/31.0                                                          |\n",
      "|\"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"                                   |\n",
      "|null                                                                                                                                       |\n",
      "|\"Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_1 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D201 Safari/9537.53\"|\n",
      "|\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"                            |\n",
      "|Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0                                                                   |\n",
      "|Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)                                                                            |\n",
      "|Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)                                                                    |\n",
      "|\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"                  |\n",
      "|\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_2) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/537.75.14\"                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for different userAgent\n",
    "spark.sql('''\n",
    "select distinct userAgent\n",
    "from t1\n",
    "''').show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### clean userAgent column\n",
    "\n",
    "Detailed sub-categories on userAgent is not useful for aggregation and analytical purpose. Instead We will only keep system information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"\"\n",
    "df=df.withColumn(\"userAgent\",regexp_replace('userAgent','\"',''))\n",
    "# remove Mozilla/5.0 \n",
    "df=df.withColumn(\"userAgent\",regexp_replace('userAgent','Mozilla/5.0 ',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize userAgent\n",
    "regexTokenizer =RegexTokenizer(inputCol='userAgent',outputCol='words',pattern=\"\\\\W\")\n",
    "df=regexTokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep first element in words column\n",
    "df=df.withColumn('OS',df.words[0].cast(StringType())).drop('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='(Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30', OS='windows')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT userAgent)|\n",
      "+-------------------------+\n",
      "|                       56|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('t1')\n",
    "spark.sql('''\n",
    "select count(distinct userAgent)\n",
    "from t1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### check missing values for column userId and sessionId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+---+\n",
      "|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId|song|status| ts|userAgent|userId| OS|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+---+\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# records without userids or sessionids\n",
    "spark.sql('''\n",
    "select *\n",
    "from t1\n",
    "where sessionId is null or userId is null''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are no Nan values in either column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|      |\n",
      "|    10|\n",
      "|   100|\n",
      "|100001|\n",
      "|100002|\n",
      "|100003|\n",
      "|100004|\n",
      "|100005|\n",
      "|100006|\n",
      "|100007|\n",
      "|100008|\n",
      "|100009|\n",
      "|100010|\n",
      "|100011|\n",
      "|100012|\n",
      "|100013|\n",
      "|100014|\n",
      "|100015|\n",
      "|100016|\n",
      "|100017|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check empty userids\n",
    "spark.sql('''\n",
    "select distinct userId\n",
    "from t1\n",
    "order by 1''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> However there are empty userids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+---+\n",
      "|artist|auth|firstName|gender|itemInSession|lastName|length|level|location|method|page|registration|sessionId|song|status| ts|userAgent|userId| OS|\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+---+\n",
      "+------+----+---------+------+-------------+--------+------+-----+--------+------+----+------------+---------+----+------+---+---------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# empty userid recrods characteristics\n",
    "df.createOrReplaceTempView('t1')\n",
    "spark.sql('''\n",
    "select *\n",
    "from t1\n",
    "where userId =\"\"\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "select distinct auth, page\n",
    "from t1\n",
    "where userId =\"\"\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Empty userId records have majority features as NaN, uers didn't listen to any music. We will drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278154"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.sql('''\n",
    "select *\n",
    "from t1\n",
    "where userId !=\"\"\n",
    "''')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### clean location column\n",
    "\n",
    "Location provides detailed sub-categories, which doesn't provide district level geographic data distribution. Instead we will keep the state abbreviation only so that we can aggregate data by states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check different locations\n",
    "spark.sql('''\n",
    "select distinct location\n",
    "from t1\n",
    "''').show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distinct amount\n",
    "spark.sql('''\n",
    "select count(distinct location)\n",
    "from t1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep state abb only\n",
    "state=udf(lambda x: x.strip().split(', ')[-1])\n",
    "df=df.withColumn('location',state(df.location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distinct amount of state abb\n",
    "df.select('location').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('t1')\n",
    "print(\"amount of people stay (without Cancellation Confirmation status)\")\n",
    "spark.sql('''\n",
    "select count(distinct userId)\n",
    "from t1\n",
    "where userId not in (\n",
    "select distinct userId\n",
    "from t1\n",
    "where page=\"Cancellation Confirmation\")\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"amount of people chruned (with Cancellation Confirmation status)\")\n",
    "spark.sql('''\n",
    "select count(distinct userId)\n",
    "from t1\n",
    "where page=\"Cancellation Confirmation\"\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag 'Cancellation Confirmation' status\n",
    "df.createOrReplaceTempView('t1')\n",
    "spark.udf.register(\"churn\", lambda x: 1 if x==\"Cancellation Confirmation\" else 0, IntegerType())\n",
    "df=spark.sql('''\n",
    "select *, sum(churn(page))over(partition by userId) Churn\n",
    "from t1\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check result\n",
    "df.createOrReplaceTempView('t1')\n",
    "spark.sql('''\n",
    "select Churn, count(*)\n",
    "from t1\n",
    "group by 1''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "233290/44864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Our label on Churn is 1:5.2. This may cause imblance data problem for mechine learning procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Churn rate vs gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"churnsex\")\n",
    "churnsex=spark.sql('''\n",
    "select gender, avg(Churn) churn_rate\n",
    "from (\n",
    "select distinct userId,gender,Churn\n",
    "from churnsex) temp\n",
    "group by 1\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churnsex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(churnsex['gender'],100*churnsex['churn_rate'])\n",
    "plt.title('Churn rate vs gender');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that male churn rate is higher than female. 26.4% VS. 19.2% to be specific.\n",
    "\n",
    "###### churn rate vs location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"chrunstate\")\n",
    "chrunstate=spark.sql('''\n",
    "select location, avg(Churn) churn_rate\n",
    "from (\n",
    "    select distinct userId,location,Churn\n",
    "    from chrunstate) temp\n",
    "group by 1\n",
    "having avg(Churn)>0\n",
    "order by 2 desc, 1\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user amount vs churn user amount every location\n",
    "chruncountstate=spark.sql('''\n",
    "select distinct userId,location,Churn\n",
    "from chrunstate\n",
    "order by 2\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(211)\n",
    "sns.barplot(100*chrunstate['churn_rate'],chrunstate['location'],color='c')\n",
    "plt.title('churn rate vs location')\n",
    "\n",
    "plt.subplot(212)\n",
    "sns.countplot(data=chruncountstate,x='location',hue='Churn')\n",
    "plt.title('total uers and total churn per location')\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we can see churn ratio per location as well as uers/churn users count per location. One thing worth to notice, even though some of the state has a high churn ratio, it might due to the fact the user base is really small. Vice versa. \n",
    "\n",
    "> E.g. AR's churn rate is 100% because it has a total amount of 1 users while 1 churned.\n",
    "\n",
    "###### Churn status vs avg amount of songs listened\n",
    "###### Churn status vs avg amount of listen time (in hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Churn status vs avg amount of songs listened\n",
    "df.createOrReplaceTempView('avgsong')\n",
    "avgsongpersuer=spark.sql('''\n",
    "select Churn, count(song)/count(distinct userId) avgsongpersuer\n",
    "from avgsong\n",
    "group by 1''').toPandas()\n",
    "avgsongpersuer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn status vs avg amount of listen time (in hours)\n",
    "avgtime=spark.sql('''\n",
    "select Churn, sum(length)/count(distinct userId)/60/60 avgtimepersuer\n",
    "from avgsong\n",
    "group by 1''').toPandas()\n",
    "avgtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casster\n",
    "hrssongs=spark.sql('''\n",
    "select distinct userId, Churn, round(sum(length)/60/60,2) hrs, count(song) songs\n",
    "from avgsong\n",
    "group by 1,2''').toPandas()\n",
    "hrssongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "sns.barplot(avgsongpersuer['Churn'],avgsongpersuer['avgsongpersuer'])\n",
    "plt.title('Churn status vs avg song listened')\n",
    "plt.ylabel('count')\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.barplot(avgtime['Churn'],avgtime['avgtimepersuer'])\n",
    "plt.title('Churn status vs avg amount of listen time (in hours)')\n",
    "plt.ylabel(\"hours\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g=sns.FacetGrid(data=hrssongs, hue='Churn',height=5)\n",
    "g.map(plt.scatter,'songs','hrs')\n",
    "g.add_legend()\n",
    "plt.title('songs vs listen hrs between two group');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference of amount of songs between two groups is:\",\n",
    "int(avgsongpersuer['avgsongpersuer'].max()-avgsongpersuer['avgsongpersuer'].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference of amount of listen time between two groups is {:.2f} hrs\"\n",
    "      .format(avgtime['avgtimepersuer'].max()-avgtime['avgtimepersuer'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see a trend that user who stayed listend 408 more songs on average than the ones who churned. User who stayed spent 28.38 hours more than user who churned on listening musics. Scatter plot indicates the relationships.\n",
    "\n",
    "###### Song length proportional distribution by churn group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churnedlen=spark.sql('''\n",
    "select length\n",
    "from avgsong\n",
    "where Churn=1''').toPandas()\n",
    "\n",
    "staylen=spark.sql('''\n",
    "select length\n",
    "from avgsong\n",
    "where Churn=0''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(churnedlen,label='churn',hist_kws={'alpha':1})\n",
    "sns.distplot(staylen,label='stay',hist_kws={'alpha':.5})\n",
    "plt.legend(loc=0)\n",
    "plt.title('song length distribution by churn group')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In term of proportional distributions of lengths of songs listened by two cohorts, they are very similar.\n",
    "###### Status 404 distribution between cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status=spark.sql('''\n",
    "select status, Churn, count(status)/count(distinct userId) ratio\n",
    "from avgsong\n",
    "where status='404'\n",
    "group by 1,2''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=status,x='status',y='ratio',hue='Churn')\n",
    "plt.ylabel('count')\n",
    "plt.title('avg 404 received per user');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To inspect if user churn due to HTTP technical reason, we can check average 404 message received per user in each group. User who stayed receive more than 2 per person while user churned receive less than 1.5. This indicates status may not be a strong causation.\n",
    "###### Page distributions in each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=spark.sql('''\n",
    "select Churn,page, count(userId)/count(distinct userId) avg_interaction\n",
    "from avgsong\n",
    "group by 1,2\n",
    "order by 2,1''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y=page['page'],x=page['avg_interaction'],hue=page['Churn'])\n",
    "plt.xscale('log')\n",
    "locs, labels=plt.xticks()\n",
    "plt.xticks(ticks=locs,labels=locs)\n",
    "plt.xlim(.1,1500)\n",
    "plt.title('Avg Page Interaction Per User')\n",
    "plt.xlabel('avg_interaction times');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Roll Advert difference between two cohorts is :\",page.query(\"page=='Roll Advert'\")['avg_interaction'].diff()[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Upgrade difference between two cohorts is :\",page.query(\"page=='Upgrade'\")['avg_interaction'].diff()[35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Observing 'Avg Page Interaction Per User' plot, we can gain some interesting insights. Gerenally speaking, stayed users interact with app more than churned users. But we can see four page categories churned users interact more than stayed users. They are:\n",
    "* Cancel\n",
    "* Cancellation Confirmation\n",
    "* Roll Advert\n",
    "* Upgrade\n",
    "\n",
    "> Regardless of cancel interactions, churned users have 0.45 more roll advert page than stayed user on average, could they churned because they received too many ads? Another interesting fact is churned uer land on Upgrade page more than stayed users even though the difference is really small (around 0.07). This might suggest more customers have considered upgrade account in churn cohort than stay cohort. But when we look at Submit Upgrade page interactions, stay cohort still higher than churned cohort, means convert rate of stayed users is higher than churned users, which make sense.\n",
    "###### Operating system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.toPandas().sort_values(by=['userId','sessionId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
